{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6rpYJtLx4Wi",
        "outputId": "514e9015-7e99-4d08-b34a-71a936e2ffde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries\n",
        "import pandas as pd;\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Kd7SS6xXyPGH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the file path\n",
        "file_path=\"/content/drive/My Drive/CSV/diabetes.csv\"\n",
        "dataset=pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "V8lkyGbAyU34"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset here\n",
        "dataset = pd.read_csv(file_path, header=None).values"
      ],
      "metadata": {
        "id": "Y_8vQ1VF0bZ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(dataset[:,0:8], dataset[:,8],\n",
        "                                                    test_size=0.25, random_state=87)\n",
        "np.random.seed(155)\n",
        "#Creating the model\n",
        "my_first_nn = Sequential()\n",
        "\n",
        "#Adding Hidden Layer with relu activation function\n",
        "my_first_nn.add(Dense(20, input_dim=8, activation='relu'))\n",
        "\n",
        "#Adding the output layer with sigmoid activation function\n",
        "my_first_nn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compiling and fitting the model\n",
        "my_first_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "my_first_nn.fit(X_train, Y_train, epochs=100,initial_epoch=0)\n",
        "\n",
        "#Printing out the output\n",
        "loss,accuracy= my_first_nn.evaluate(X_test, Y_test)\n",
        "print(\"Loss:\",loss)\n",
        "print(\"Accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQmOFwuM03wo",
        "outputId": "309a2970-ba51-4f7d-b851-df50acf0239b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 5s 3ms/step - loss: 20.5973 - acc: 0.3385\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 10.6136 - acc: 0.3385\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 5.9158 - acc: 0.4132\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 4.0659 - acc: 0.4618\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 3.1753 - acc: 0.4601\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.8228 - acc: 0.4740\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.4376 - acc: 0.5069\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1931 - acc: 0.5104\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8598 - acc: 0.5469\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.5465 - acc: 0.5972\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3403 - acc: 0.6146\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2056 - acc: 0.6267\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1075 - acc: 0.6302\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0459 - acc: 0.6302\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9716 - acc: 0.6562\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9324 - acc: 0.6597\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8926 - acc: 0.6510\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8541 - acc: 0.6545\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8416 - acc: 0.6493\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8135 - acc: 0.6562\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7879 - acc: 0.6528\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7732 - acc: 0.6649\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7566 - acc: 0.6424\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7493 - acc: 0.6580\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7378 - acc: 0.6684\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7395 - acc: 0.6753\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7248 - acc: 0.6753\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7156 - acc: 0.6701\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6899 - acc: 0.6823\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6892 - acc: 0.6840\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7022 - acc: 0.6771\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7302 - acc: 0.6701\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6781 - acc: 0.6684\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6771 - acc: 0.7014\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6707 - acc: 0.6875\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6786 - acc: 0.6753\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6472 - acc: 0.7101\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6753 - acc: 0.6927\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6879 - acc: 0.6788\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6715 - acc: 0.6944\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6405 - acc: 0.7101\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6306 - acc: 0.7170\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6560 - acc: 0.6840\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6241 - acc: 0.7014\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6298 - acc: 0.7014\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6432 - acc: 0.7014\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6250 - acc: 0.7049\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6366 - acc: 0.6962\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6078 - acc: 0.7240\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6345 - acc: 0.7118\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6113 - acc: 0.7222\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6094 - acc: 0.7083\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6191 - acc: 0.7049\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6040 - acc: 0.7049\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6004 - acc: 0.7188\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6000 - acc: 0.7014\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6239 - acc: 0.6806\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6164 - acc: 0.6979\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5947 - acc: 0.7135\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6048 - acc: 0.7135\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6201 - acc: 0.6979\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6239 - acc: 0.6823\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5976 - acc: 0.7066\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5796 - acc: 0.7326\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6002 - acc: 0.7049\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6456 - acc: 0.7014\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5954 - acc: 0.7257\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5846 - acc: 0.7326\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5734 - acc: 0.7309\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5893 - acc: 0.7118\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5861 - acc: 0.7205\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5741 - acc: 0.7309\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5809 - acc: 0.7101\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6623 - acc: 0.6910\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6001 - acc: 0.7031\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5935 - acc: 0.7135\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5770 - acc: 0.7188\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5894 - acc: 0.7188\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6080 - acc: 0.7101\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5563 - acc: 0.7344\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5954 - acc: 0.6979\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5979 - acc: 0.7066\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5751 - acc: 0.7257\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6295 - acc: 0.6927\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6392 - acc: 0.7135\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6572 - acc: 0.6910\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5768 - acc: 0.7066\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5735 - acc: 0.7257\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5775 - acc: 0.7274\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5666 - acc: 0.7240\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5550 - acc: 0.7274\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5552 - acc: 0.7222\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5573 - acc: 0.7309\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5574 - acc: 0.7396\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5729 - acc: 0.7170\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5967 - acc: 0.6997\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5978 - acc: 0.7153\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5806 - acc: 0.7135\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6508 - acc: 0.6892\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6256 - acc: 0.6944\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.7200 - acc: 0.6562\n",
            "Loss: 0.7200057506561279\n",
            "Accuracy: 0.65625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(dataset[:,0:8], dataset[:,8], test_size=0.25, random_state=87)\n",
        "np.random.seed(155)\n",
        "\n",
        "# Creating the model\n",
        "my_first_nn = Sequential()\n",
        "\n",
        "# Adding anonther hidden layer with the relu activation function\n",
        "my_first_nn.add(Dense(20, input_dim=8, activation='relu'))\n",
        "\n",
        "# Adding another Hidden layer with the tanh activation function\n",
        "my_first_nn.add(Dense(20, activation='tanh'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Adding another Hidden layer with the sigmoid activation function\n",
        "my_first_nn.add(Dense(20, activation='sigmoid'))\n",
        "\n",
        "# Output Layer with sigmoid activation function\n",
        "my_first_nn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compiling and fitting the model\n",
        "my_first_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "my_first_nn.fit(X_train, Y_train, epochs=100, initial_epoch=0)\n",
        "\n",
        "loss,accuracy = my_first_nn.evaluate(X_test, Y_test)\n",
        "#Printing out the output\n",
        "print(\"Loss:\",loss)\n",
        "print(\"Accuracy:\",accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY-dT2DW1VQo",
        "outputId": "e38b7130-1a68-48ec-f78f-5d6ddf220ea5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 2s 3ms/step - loss: 0.6423 - acc: 0.6615\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6352 - acc: 0.6615\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6316 - acc: 0.6615\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6286 - acc: 0.6615\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6263 - acc: 0.6615\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6234 - acc: 0.6615\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6180 - acc: 0.6615\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6135 - acc: 0.6615\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6172 - acc: 0.6615\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6092 - acc: 0.6615\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6032 - acc: 0.6615\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6001 - acc: 0.6615\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5962 - acc: 0.6615\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5915 - acc: 0.6632\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5950 - acc: 0.6649\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5841 - acc: 0.6753\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5783 - acc: 0.6788\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5747 - acc: 0.6736\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5761 - acc: 0.6667\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5749 - acc: 0.6684\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5843 - acc: 0.6649\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5856 - acc: 0.6736\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5728 - acc: 0.6910\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5714 - acc: 0.7014\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5672 - acc: 0.6944\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5674 - acc: 0.6892\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5640 - acc: 0.6979\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5645 - acc: 0.7066\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5579 - acc: 0.7118\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5646 - acc: 0.6979\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5634 - acc: 0.6927\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5698 - acc: 0.6979\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5691 - acc: 0.6962\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5660 - acc: 0.6979\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5655 - acc: 0.6997\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5596 - acc: 0.6910\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5623 - acc: 0.7083\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5594 - acc: 0.7066\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5556 - acc: 0.7083\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5545 - acc: 0.6997\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5590 - acc: 0.7066\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5532 - acc: 0.7257\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5497 - acc: 0.7240\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5502 - acc: 0.7240\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5547 - acc: 0.7066\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5544 - acc: 0.7066\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5508 - acc: 0.7222\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5648 - acc: 0.7257\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5555 - acc: 0.7170\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5546 - acc: 0.7066\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5510 - acc: 0.7101\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5503 - acc: 0.7170\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5522 - acc: 0.7031\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5548 - acc: 0.7101\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5517 - acc: 0.7153\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5473 - acc: 0.7066\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5526 - acc: 0.7118\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5434 - acc: 0.7170\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5463 - acc: 0.7188\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5550 - acc: 0.7066\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5464 - acc: 0.7135\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5447 - acc: 0.7049\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5522 - acc: 0.7083\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5523 - acc: 0.7066\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5441 - acc: 0.7083\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5411 - acc: 0.7118\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5424 - acc: 0.7118\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5416 - acc: 0.7083\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5394 - acc: 0.7049\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5387 - acc: 0.7101\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5364 - acc: 0.7135\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5397 - acc: 0.7222\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5319 - acc: 0.7153\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5326 - acc: 0.7257\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5369 - acc: 0.7222\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5391 - acc: 0.7118\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5286 - acc: 0.7170\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5413 - acc: 0.7101\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5323 - acc: 0.7274\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5275 - acc: 0.7309\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5269 - acc: 0.7292\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5291 - acc: 0.7326\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5421 - acc: 0.7031\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5357 - acc: 0.7135\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5268 - acc: 0.7205\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5267 - acc: 0.7257\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5362 - acc: 0.7153\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5290 - acc: 0.7101\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5233 - acc: 0.7170\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5216 - acc: 0.7240\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5301 - acc: 0.7135\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5198 - acc: 0.7309\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5358 - acc: 0.7188\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5219 - acc: 0.7240\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5223 - acc: 0.7257\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5180 - acc: 0.7205\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5202 - acc: 0.7292\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5207 - acc: 0.7205\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5230 - acc: 0.7222\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5183 - acc: 0.7170\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.5654 - acc: 0.7396\n",
            "Loss: 0.5653932094573975\n",
            "Accuracy: 0.7395833134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFVF9V5J4eKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "Hx4wb69L6Ch6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images,train_labels),(test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(train_images.shape[1:])\n",
        "#process the data\n",
        "#Converting each image of shape (28*28) to 784 dimensional which will be fed to the network as a single feature\n",
        "dimData = np.prod(train_images.shape[1:])\n",
        "print(dimData)\n",
        "train_data = train_images.reshape(train_images.shape[0],dimData)\n",
        "test_data = test_images.reshape(test_images.shape[0],dimData)\n",
        "\n",
        "#convert data to float and scale values between 0 and 1\n",
        "train_data = train_data.astype('float')\n",
        "test_data = test_data.astype('float')\n",
        "#scale data\n",
        "train_data /=255.0\n",
        "test_data /=255.0\n",
        "#change the labels frominteger to one-hot encoding. to_categorical is doing the same thing as LabelEncoder()\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "#creating network\n",
        "model = Sequential()\n",
        "#Adding the hidden layer with relu activation function\n",
        "model.add(Dense(512, activation='relu', input_shape=(dimData,)))\n",
        "\n",
        "#Adding another hidden layer with tanh activation function\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "\n",
        "#Adding another hidden layer again with tanh activation function\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#Compiling and fitting the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=10, verbose=1,validation_data=(test_data, test_labels_one_hot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87oo6lHv6cd_",
        "outputId": "11d5657d-a564-41ab-c872-1315f3d2687a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "784\n",
            "Epoch 1/10\n",
            "235/235 [==============================] - 3s 7ms/step - loss: 0.2815 - accuracy: 0.9123 - val_loss: 0.1603 - val_accuracy: 0.9513\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0957 - accuracy: 0.9705 - val_loss: 0.1163 - val_accuracy: 0.9649\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0617 - accuracy: 0.9806 - val_loss: 0.1171 - val_accuracy: 0.9656\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0433 - accuracy: 0.9860 - val_loss: 0.1008 - val_accuracy: 0.9712\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0318 - accuracy: 0.9895 - val_loss: 0.0823 - val_accuracy: 0.9747\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.0733 - val_accuracy: 0.9798\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.0769 - val_accuracy: 0.9809\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0740 - val_accuracy: 0.9806\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0810 - val_accuracy: 0.9799\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0872 - val_accuracy: 0.9802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images,train_labels),(test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(train_images.shape[1:])\n",
        "#process the data\n",
        "#Converting each image of shape (28*28) to 784 dimensional which will be fed to the network as a single feature\n",
        "dimData = np.prod(train_images.shape[1:])\n",
        "print(dimData)\n",
        "train_data = train_images.reshape(train_images.shape[0],dimData)\n",
        "test_data = test_images.reshape(test_images.shape[0],dimData)\n",
        "\n",
        "#convert data to float and scale values between 0 and 1\n",
        "train_data = train_data.astype('float')\n",
        "test_data = test_data.astype('float')\n",
        "\n",
        "#change the labels frominteger to one-hot encoding. to_categorical is doing the same thing as LabelEncoder()\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "#creating network\n",
        "model = Sequential()\n",
        "#Adding the hidden layer with relu activation function\n",
        "model.add(Dense(512, activation='relu', input_shape=(dimData,)))\n",
        "\n",
        "#Adding another hidden layer with tanh activation function\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "\n",
        "#Adding another hidden layer again with tanh activation function\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#Compiling and fitting the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=10, verbose=1,validation_data=(test_data, test_labels_one_hot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YZmZKGs7ki4",
        "outputId": "e3a766f0-06fc-493a-b730-08b998349e95"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "784\n",
            "Epoch 1/10\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.5874 - accuracy: 0.8263 - val_loss: 0.2748 - val_accuracy: 0.9160\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2698 - accuracy: 0.9155 - val_loss: 0.2565 - val_accuracy: 0.9184\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2202 - accuracy: 0.9318 - val_loss: 0.2075 - val_accuracy: 0.9387\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1948 - accuracy: 0.9398 - val_loss: 0.2490 - val_accuracy: 0.9194\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1832 - accuracy: 0.9428 - val_loss: 0.2820 - val_accuracy: 0.9090\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.1708 - accuracy: 0.9468 - val_loss: 0.2023 - val_accuracy: 0.9411\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1624 - accuracy: 0.9497 - val_loss: 0.1813 - val_accuracy: 0.9442\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1487 - accuracy: 0.9533 - val_loss: 0.1459 - val_accuracy: 0.9523\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1346 - accuracy: 0.9575 - val_loss: 0.1686 - val_accuracy: 0.9492\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1345 - accuracy: 0.9581 - val_loss: 0.1370 - val_accuracy: 0.9583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose an image from the test data (e.g., the first image, index 0)\n",
        "image_to_predict = test_data[0]\n",
        "true_label = test_labels[0]\n",
        "\n",
        "# Reshape the image to its original 28x28 shape (for visualization)\n",
        "original_image = test_images[0]\n",
        "\n",
        "# Plot the original image\n",
        "plt.imshow(original_image, cmap='gray')\n",
        "plt.title(f'True Label: {true_label}')\n",
        "plt.show()\n",
        "\n",
        "# Make a prediction on the chosen image\n",
        "prediction = model.predict(np.expand_dims(image_to_predict, axis=0))\n",
        "predicted_label = np.argmax(prediction)\n",
        "\n",
        "# Print the prediction\n",
        "print(f'Predicted Label: {predicted_label}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "jxzFbnXk8Es9",
        "outputId": "4ecc0262-d7ec-4e7b-d642-718601d45d59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 230ms/step\n",
            "Predicted Label: 7\n"
          ]
        }
      ]
    }
  ]
}